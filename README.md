# Proyecto de Data Engineering con la API de Spotify

[![Infra Provision](https://github.com/jesusoviedo/spotify-dwh-insights/actions/workflows/ci-cd-terraform.yml/badge.svg)](https://github.com/jesusoviedo/spotify-dwh-insights/actions/workflows/ci-cd-terraform.yml)
[![CI-CD-Deploy-Kestra-Instance](https://github.com/jesusoviedo/spotify-dwh-insights/actions/workflows/ci-cd-deploy-instance.yml/badge.svg)](https://github.com/jesusoviedo/spotify-dwh-insights/actions/workflows/ci-cd-deploy-instance.yml)
[![CI-CD-Deploy-Kestra-Flows](https://github.com/jesusoviedo/spotify-dwh-insights/actions/workflows/ci-cd-deploy-flows.yml/badge.svg)](https://github.com/jesusoviedo/spotify-dwh-insights/actions/workflows/ci-cd-deploy-flows.yml)

[![Build & Deploy Docker](https://github.com/jesusoviedo/spotify-dwh-insights/actions/workflows/ci-cd-build-deploy-docker-image.yml/badge.svg)](https://github.com/jesusoviedo/spotify-dwh-insights/actions/workflows/ci-cd-build-deploy-docker-image.yml)
[![Secrets Scan](https://github.com/jesusoviedo/spotify-dwh-insights/actions/workflows/ci-sec-secrets-scan.yml/badge.svg)](https://github.com/jesusoviedo/spotify-dwh-insights/actions/workflows/ci-sec-secrets-scan.yml)
[![Code Style & Quality](https://github.com/jesusoviedo/spotify-dwh-insights/actions/workflows/ci-qa-pre-commit.yml/badge.svg)](https://github.com/jesusoviedo/spotify-dwh-insights/actions/workflows/ci-qa-pre-commit.yml)

[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)

Este proyecto de Data Engineering tiene como objetivo extraer informaciÃ³n de la API de Spotify sobre los lanzamientos recientes de Ã¡lbumes, obtener detalles de sus pistas y los datos de los artistas involucrados. 

Luego, estos datos serÃ¡n almacenados en un Data Lake, procesados para un Data Warehouse y utilizados en un dashboard con visualizaciones clave.


## ğŸ¯ Objetivos
**1. ExtracciÃ³n de datos:** obtener informaciÃ³n de Ã¡lbumes, pistas y artistas desde la API de Spotify.

**2. Almacenamiento en Data Lake:** guardar los datos sin procesar en Google Cloud Storage (GCS).

**3. Procesamiento y transformaciÃ³n:**
- Usar DLT para ingerir datos de manera eficiente.
- Aplicar dbt en BigQuery para modelar los datos.

**4. CreaciÃ³n de visualizaciones:** construir un dashboard con dos grÃ¡ficos relevantes (a definir) utilizando Looker Studio como herramienta de visualizaciÃ³n.

**5. Infraestructura como cÃ³digo:** utilizar Terraform para desplegar recursos en GCP.

**6.AutomatizaciÃ³n del pipeline:** orquestar el flujo de datos con Kestra y automatizar despliegues con GitHub Actions.


## ğŸ›ï¸ Arquitectura

![Diagrama de Arquitectura](scripts/resources/architecture_diagram.png)

1. Ingesta de datos
    - Se extraen los datos desde la API de Spotify usando DLT.
    - Los datos se almacenan en GCS como archivos Parquet.

2. Procesamiento y modelado
    - Se trasladan los datos desde GCS a BigQuery.
    - Se aplican transformaciones y modelos con dbt.

3. VisualizaciÃ³n
    - Se usa Looker Studio como herramienta de BI para crear un Dashboard interativo con dos pÃ¡ginas.


## ğŸ› ï¸ TecnologÃ­as utilizadas
- **Python:** Lenguaje principal para construir el pipeline.
- **Docker:** ContenerizaciÃ³n del pipeline para facilitar la portabilidad y consistencia.
- **Docker Hub:** Almacenamiento y distribuciÃ³n de las imÃ¡genes del pipeline.
- **Kestra:** OrquestaciÃ³n del pipeline de datos.
- **DLT:** ExtracciÃ³n e ingestiÃ³n de datos.
- **Google Cloud Storage (GCS):** Almacenamiento del Data Lake.
- **BigQuery:** Data Warehouse.
- **DBT:** Modelado de datos.
- **Looker Studio:** VisualizaciÃ³n de datos.
- **Terraform:** Infraestructura como cÃ³digo en GCP.
- **GitHub Actions:** CI/CD para despliegue en producciÃ³n.

## ğŸ“‚ Estructura de carpetas
```bash
ğŸ“‚ spotify-data-engineering  
â”‚â”€â”€ ğŸ“„ .github/workflows        # ConfiguraciÃ³n de GitHub Actions 
â”‚â”€â”€ ğŸ“‚ data/                    # Archivos de datos de ejemplo 
â”‚â”€â”€ ğŸ“‚ dbt/                     # Modelos de transformaciÃ³n para BigQuery  
â”‚â”€â”€ ğŸ“‚ dlt/                     # ConfiguraciÃ³n y scripts de DLT 
â”‚â”€â”€ ğŸ“‚ docs/                    # DocumentaciÃ³n adicional y archivos README auxiliares
â”‚â”€â”€ ğŸ“‚ kestra/                  # Flujos de trabajo en Kestra  
â”‚â”€â”€ ğŸ“‚ scripts/                 # Scripts auxiliares  
â”‚â”€â”€ ğŸ“‚ terraform/               # Definiciones de infraestructura en GCP  
â”‚â”€â”€ ğŸ“‚ visualizations/          # Dashboards y reportes  
â”‚â”€â”€ ğŸ“„ .gitignore               # Archivos a excluir del repositorio
â”‚â”€â”€ ğŸ“„ .pre-commit-config.yaml  # ConfiguraciÃ³n de pre-commit para hooks y linters
â”‚â”€â”€ ğŸ“„ README.md                # DocumentaciÃ³n del proyecto  
â”‚â”€â”€ ğŸ“„ LICENSE                  # InformaciÃ³n sobre la licencia del proyecto 
```


## â˜ï¸ ConfiguraciÃ³n previa en GCP
Antes de ejecutar este proyecto de manera local o desplegarlo en GCP, es necesario configurar correctamente el entorno en Google Cloud Platform (GCP). Esto incluye:

1. Crear un proyecto en GCP
2. Configurar cuentas de servicio con los permisos adecuados:
- Una cuenta de servicio para gestionar el almacenamiento en Google Cloud Storage (GCS).
- Una cuenta de servicio para administrar datasets y tablas en BigQuery.
- Una cuenta de servicio para desplegar instancias en Compute Engine (opcional, si se usarÃ¡ Docker en GCE).

Para una guÃ­a detallada sobre estos pasos, consulta el archivo [`setup_gcp_project`](./docs/setup_gcp_project.md).


## âš™ï¸ ConfiguraciÃ³n del entorno local


### ğŸ Entorno de Python con Pipenv
Para gestionar el entorno de Python en este proyecto, se utiliza Pipenv, que permite crear un entorno virtual, instalar dependencias y ejecutar cÃ³digo de manera aislada.

No todas las carpetas del proyecto requieren un entorno virtual. En aquellas donde sea necesario, se incluirÃ¡n los pasos especÃ­ficos para configurarlo con Pipenv.

A continuaciÃ³n, se resumen los pasos bÃ¡sicos:
- InstalaciÃ³n de Pipenv
- CreaciÃ³n del entorno e instalaciÃ³n de dependencias
- ActivaciÃ³n del entorno virtual
- EjecuciÃ³n de cÃ³digo dentro del entorno

Para ver los detalles y comandos especÃ­ficos, consulta el archivo [`python_env_setup.md`](./docs/python_env_setup.md).


### ğŸ³ Docker y Docker Compose
Este proyecto requiere Docker y Docker Compose para ejecutarse de manera local. Estas herramientas permiten contenerizar los servicios, garantizando un entorno reproducible e independiente del sistema operativo.

Si necesitas una referencia rÃ¡pida sobre Docker, consulta el archivo [`docker_basics.md`](./docs/docker_basics.md), donde encontrarÃ¡s informaciÃ³n sobre la instalaciÃ³n, gestiÃ³n de contenedores e imÃ¡genes.


### ğŸŒ Terraform
Este proyecto requiere Terraform instalado para poder gestionar y desplegar la infraestructura en la nube de manera local. Terraform permite crear, actualizar y administrar los recursos de infraestructura de forma eficiente y reproducible.

Si necesitas una referencia rÃ¡pida sobre cÃ³mo instalar y utilizar Terraform, consulta el archivo [`terraform_basics.md`](./docs/terraform_basics.md), donde encontrarÃ¡s informaciÃ³n sobre la instalaciÃ³n, configuraciÃ³n y comandos bÃ¡sicos de Terraform.


## ğŸ“‚ OrganizaciÃ³n del Proyecto
A continuaciÃ³n, se describe el contenido de cada carpeta principal dentro del proyecto para facilitar su comprensiÃ³n y uso.


### ğŸ“Š Entendiendo los Datos ([`data/`](./data))
Esta carpeta contiene archivos de datos de ejemplo utilizados para pruebas y informacion del servicio de Spotify. AquÃ­ se incluyen respuestas de la API de Spotify en formato JSON, que sirven como referencia para entender la estructura de los datos obtenidos.

âš ï¸ Uso de los datos: este proyecto no almacena datos de manera permanente. Los archivos en esta carpeta son solo ejemplos de respuestas de la API de Spotify y no contienen datos en tiempo real. Para mÃ¡s informaciÃ³n sobre las restricciones de uso de los datos y los servicios de Spotify, consulta el archivo `data/README.md`.


### ğŸ“š DocumentaciÃ³n Auxiliar ([`docs/`](./docs))
Esta carpeta contiene guÃ­as rÃ¡pidas y documentaciÃ³n de referencia para configurar y entender herramientas clave utilizadas en el proyecto.


### ğŸ›  Scripts Auxiliares ([`scripts/`](./scripts))
AquÃ­ se encuentran scripts en Python diseÃ±ados para facilitar la automatizaciÃ³n de diversas tareas dentro del proyecto. Estos pueden incluir herramientas de soporte, pruebas y procesos adicionales relacionados con la gestiÃ³n de datos.


### â˜ï¸ Infraestructura como CÃ³digo ([`terraform/`](./terraform))
Esta carpeta contiene las configuraciones de Terraform necesarias para desplegar y gestionar los recursos en Google Cloud Platform (GCP). AquÃ­ se definen los servicios en la nube de forma eficiente, reproducible y automatizada.


### ğŸš€ ExtracciÃ³n e Ingesta con DLT ([`dlt/`](./dlt))
Esta carpeta contiene la configuraciÃ³n y los scripts necesarios para extraer datos desde la API de Spotify y cargarlos en el Data Lake de GCS.


### ğŸ”„ OrquestaciÃ³n con Kestra ([`kestra/`](./kestra))
En esta carpeta se encuentran los flujos de trabajo definidos en Kestra. AquÃ­ se configura la ejecuciÃ³n automatizada del pipeline, incluyendo la extracciÃ³n, transformaciÃ³n y carga de datos.


### ğŸ— Modelado de Datos con DBT ([`dbt/`](./dbt))
AquÃ­ se encuentran los modelos de transformaciÃ³n creados con dbt para estructurar los datos almacenados en BigQuery. Contiene definiciones de modelos, fuentes de datos, pruebas y documentaciÃ³n generada automÃ¡ticamente.


### ğŸ“ˆ VisualizaciÃ³n de Datos con Looker Studio ([`visualizations/`](./visualizations))
Esta carpeta contiene los recursos relacionados con la creaciÃ³n de dashboards interactivos en Looker Studio. Incluye configuraciones y documentaciÃ³n del reporte desarrollado, el cual se actualiza automÃ¡ticamente cada 12 horas o bajo demanda. Actualmente, el dashboard cuenta con dos pÃ¡ginas que ofrecen insights visuales a partir de los datos procesados, permitiendo explorar mÃ©tricas clave.


### ğŸ”„ IntegraciÃ³n y Despliegue AutomÃ¡tico ([`.github/workflows/`](./.github/workflows)`)
En esta carpeta se almacena la configuraciÃ³n de GitHub Actions para implementar un flujo de CI/CD robusto. Asegura que cada cambio en el cÃ³digo pase por validaciones antes de ser desplegado, permitiendo una entrega continua y segura.

## ğŸ§¹ Code Quality

Este proyecto utiliza [pre-commit](https://pre-commit.com/) para ejecutar linters y herramientas de formato de cÃ³digo antes de realizar un commit. Puedes obtener mÃ¡s detalles sobre cÃ³mo configurar y usar estas herramientas en el archivo [code_quality.md](./docs/code_quality.md).

## ğŸ¤ Contribuciones
Las contribuciones son bienvenidas. Si tienes alguna idea o mejora, no dudes en hacer un fork del repositorio y enviar un pull request. Toda ayuda es apreciada para mejorar el proyecto.


## ğŸ“œ Licencia
[![Licencia MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)

Este proyecto estÃ¡ licenciado bajo la Licencia MIT. Para mÃ¡s detalles, consulta el archivo [`LICENSE`](./LICENSE).


## ğŸ“¬ Contacto
Para cualquier pregunta, sugerencia o problema relacionado con el proyecto, puedes ponerte en contacto con el desarrollador principal a travÃ©s de su perfil de LinkedIn:

- **JesÃºs Oviedo Riquelme**: [LinkedIn](https://linkedin.com/in/jesusoviedoriquelme)

Si necesitas soporte adicional o tienes consultas especÃ­ficas, no dudes en enviar un mensaje a travÃ©s de esta plataforma.
